----------------- Options ---------------
               batch_size: 1                             
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ./datasets/thread             	[default: None]
             dataset_mode: aligned                       
                direction: AtoB                          
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: vanilla                       
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                lambda_L1: 100.0                         
                load_iter: 0                             	[default: 0]
                load_size: 256                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: pix2pix                       	[default: cycle_gan]
                 n_epochs: 100                           
           n_epochs_decay: 100                           
               n_layers_D: 3                             
                     name: thread_pix2pix                	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: unet_256                      
                      ngf: 64                            
               no_dropout: False                         
                  no_flip: False                         
                     norm: batch                         
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 0                             
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 10                            
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
                use_wandb: False                         
                  verbose: False                         
       wandb_project_name: CycleGAN-and-pix2pix          
----------------- End -------------------
dataset [AlignedDataset] was created
The number of training images = 16224
initialize network with normal
initialize network with normal
model [Pix2PixModel] was created
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.414 M
[Network D] Total number of parameters : 2.769 M
-----------------------------------------------
learning rate 0.0002000 -> 0.0002000
End of epoch 1 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 2 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 3 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 4 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 5 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 6 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 7 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 8 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 9 / 200 	
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 10, iters 162240
G_GAN: 2.469144174225235
G_L1: 15.416624260886298
D_real: 0.3633805484777318
D_fake: 0.31993514603321255
End of epoch 10 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 11 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 12 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 13 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 14 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 15 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 16 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 17 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 18 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 19 / 200 	
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 20, iters 324480
G_GAN: 4.336571595856563
G_L1: 15.582886754342262
D_real: 0.19286261155072945
D_fake: 0.17458123529925337
End of epoch 20 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 21 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 22 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 23 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 24 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 25 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 26 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 27 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 28 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 29 / 200 	
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 30, iters 486720
G_GAN: 5.250767953314541
G_L1: 15.66622087262806
D_real: 0.1423708793065452
D_fake: 0.1293478758694809
End of epoch 30 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 31 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 32 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 33 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 34 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 35 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 36 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 37 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 38 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 39 / 200 	
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 40, iters 648960
G_GAN: 5.917329749238983
G_L1: 15.70535105228042
D_real: 0.11836057715922205
D_fake: 0.10899006710450534
End of epoch 40 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 41 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 42 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 43 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 44 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 45 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 46 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 47 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 48 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 49 / 200 	
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 50, iters 811200
G_GAN: 6.420435570780862
G_L1: 15.774403480963535
D_real: 0.10080687078360251
D_fake: 0.09326436020843462
End of epoch 50 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 51 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 52 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 53 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 54 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 55 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 56 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 57 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 58 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 59 / 200 	
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 60, iters 973440
G_GAN: 6.840403225917576
G_L1: 15.784572917321436
D_real: 0.09118078544444876
D_fake: 0.0858818710803751
End of epoch 60 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 61 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 62 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 63 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 64 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 65 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 66 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 67 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 68 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 69 / 200 	
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 70, iters 1135680
G_GAN: 7.115075474315648
G_L1: 15.796620283775487
D_real: 0.08283878917649937
D_fake: 0.0795514143156353
End of epoch 70 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 71 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 72 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 73 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 74 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 75 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 76 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 77 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 78 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 79 / 200 	
learning rate 0.0002000 -> 0.0002000
saving the model at the end of epoch 80, iters 1297920
G_GAN: 7.366050004673235
G_L1: 15.839787662888948
D_real: 0.0777652353703547
D_fake: 0.07398637112732093
End of epoch 80 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 81 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 82 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 83 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 84 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 85 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 86 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 87 / 200 	
learning rate 0.0002000 -> 0.0002000
End of epoch 88 / 200 	
